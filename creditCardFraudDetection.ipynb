{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('C:/Users/Dell/PycharmProjects/CreditCard Fraud Detection/creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling missing values and encoding categorical values\n",
    "\n",
    "# Identifying missing values\n",
    "missingValues = dataset.isnull().sum()\n",
    "print(missingValues)\n",
    "\n",
    "# Removing rows with missing values\n",
    "print(dataset.dropna())\n",
    "\n",
    "# since the data contain only numerical inputs, encoding categorial values in not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Understanding the Data\n",
    "print(dataset.head())\n",
    "print(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary information about the dataset\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction class distribution\n",
    "count_classes = pd.value_counts(dataset['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction Class Distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Fraud and the normal dataset\n",
    "fraud = dataset[dataset['Class']==1]\n",
    "normal = dataset[dataset['Class']==0]\n",
    "print(fraud.shape,normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fraud and Normal transaction information Description\n",
    "fraud.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount per Transaction by Class\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Amount per transaction by class')\n",
    "bins = 50\n",
    "ax1.hist(fraud.Amount, bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "ax2.hist(normal.Amount, bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xlim((0, 20000))\n",
    "plt.yscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Will check Do fraudulent transactions occur more often during certain time frame ? Let us find out with a visual representation.\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Time of transaction vs Amount by class')\n",
    "ax1.scatter(fraud.Time, fraud.Amount)\n",
    "ax1.set_title('Fraud')\n",
    "ax2.scatter(normal.Time, normal.Amount)\n",
    "ax2.set_title('Normal')\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the correlation between different features and the target variable\n",
    "\n",
    "correlation = dataset.corr()['Class'].sort_values()\n",
    "\n",
    "# Print correlation coefficients\n",
    "print(correlation)\n",
    "\n",
    "# Plot correlation matrix as a heatmap\n",
    "corrFeature = correlation.index\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(dataset[corrFeature].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimenionality Reduction by Principle Component Analysis method\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset.drop('Class', axis=1)\n",
    "y = dataset['Class']\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "\n",
    "# Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort the eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "\n",
    "# Select the top k eigenvectors based on the desired number of components\n",
    "num_components = 10\n",
    "selected_eigenvectors = eigenvectors[:, sorted_indices[:num_components]]\n",
    "\n",
    "# Project the data onto the selected eigenvectors\n",
    "X_reduced = np.dot(X_centered, selected_eigenvectors)\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Get the list of available parameters\n",
    "params = dt_classifier.get_params().keys()\n",
    "\n",
    "# Print the list of available parameters\n",
    "print(params)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train different classifiers with hyperparameter tuning\n",
    "classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    if name == 'Logistic Regression':\n",
    "        param_grid = {\n",
    "            'classifier__C': [0.1, 1, 10]\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {\n",
    "            'classifier__max_depth': [None, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__criterion': ['gini', 'entropy']\n",
    "        }\n",
    "\n",
    "    # Create a pipeline for the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Perform hyperparameter tuning using RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, cv=5, n_iter=3, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the best model on the validation set\n",
    "    best_model = random_search.best_estimator_\n",
    "    val_accuracy = best_model.score(X_val, y_val)\n",
    "\n",
    "    # Evaluate the best model on the testing set\n",
    "    test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'{name}:')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store the results of each model\n",
    "results = []\n",
    "\n",
    "# Iterate over the trained models\n",
    "for name, classifier in classifiers:\n",
    "    # Predict labels on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    }\n",
    "\n",
    "    # Append the result to the results list\n",
    "    results.append(result)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Model: {result['Model']}\")\n",
    "    print(f\"Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['F1 Score']:.4f}\")\n",
    "    print(f\"ROC AUC: {result['ROC AUC']:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Compare the results and identify the most effective classifier\n",
    "best_model_result = max(results, key=lambda x: x['Accuracy'])\n",
    "print(\"Best Model:\")\n",
    "print(f\"Model: {best_model_result['Model']}\")\n",
    "print(f\"Accuracy: {best_model_result['Accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_model_result['Precision']:.4f}\")\n",
    "print(f\"Recall: {best_model_result['Recall']:.4f}\")\n",
    "print(f\"F1 Score: {best_model_result['F1 Score']:.4f}\")\n",
    "print(f\"ROC AUC: {best_model_result['ROC AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the performance metrics from the results\n",
    "models = [result['Model'] for result in results]\n",
    "accuracies = [result['Accuracy'] for result in results]\n",
    "precisions = [result['Precision'] for result in results]\n",
    "recalls = [result['Recall'] for result in results]\n",
    "f1_scores = [result['F1 Score'] for result in results]\n",
    "roc_aucs = [result['ROC AUC'] for result in results]\n",
    "\n",
    "# Plot the performance metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(models, accuracies, label='Accuracy')\n",
    "plt.plot(models, precisions, label='Precision')\n",
    "plt.plot(models, recalls, label='Recall')\n",
    "plt.plot(models, f1_scores, label='F1 Score')\n",
    "plt.plot(models, roc_aucs, label='ROC AUC')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance Comparison of Different Models')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the performance metrics from the results\n",
    "models = [result['Model'] for result in results]\n",
    "accuracies = [result['Accuracy'] for result in results]\n",
    "precisions = [result['Precision'] for result in results]\n",
    "recalls = [result['Recall'] for result in results]\n",
    "f1_scores = [result['F1 Score'] for result in results]\n",
    "roc_aucs = [result['ROC AUC'] for result in results]\n",
    "\n",
    "# Plot the performance metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies, label='Accuracy')\n",
    "plt.bar(models, precisions, label='Precision')\n",
    "plt.bar(models, recalls, label='Recall')\n",
    "plt.bar(models, f1_scores, label='F1 Score')\n",
    "plt.bar(models, roc_aucs, label='ROC AUC')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance Comparison of Different Models')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c83ba3a9c4a48792899d879b46f735cc064e383fea74913dc7b0456e36d4e99d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
